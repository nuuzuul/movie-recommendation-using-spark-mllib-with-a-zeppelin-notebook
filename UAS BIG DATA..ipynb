{
  "metadata": {
    "name": "UAS BIG DATA",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "A%spark.pyspark\r\nimport numpy as np \r\nimport pandas as pd \r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport sklearn\r\nimport random\r\nimport os\r\nfrom pyspark.sql import SparkSession\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nspark \u003d SparkSession.builder.appName(\u0027Recommender_system\u0027).getOrCreate()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\ndata \u003d spark.read.csv(\"/home/nuzul/ratings.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\ndata.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nnum_rows \u003d data.count()\r\nnum_columns \u003d len(data.columns)\r\nprint(\"Number of rows:\", num_rows)\r\nprint(\"Number of columns:\", num_columns)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nmovie_names_data \u003d spark.read.csv(\"/home/nuzul/movies.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\nmovie_names_data.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\ndata.printSchema()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\ndata.describe().show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nimport matplotlib.pyplot as plt\r\nfrom pyspark.sql.functions import avg  \r\n\r\navg_ratings \u003d data.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\r\ncombined_data \u003d avg_ratings.join(movie_names_data, on\u003d\"movieId\")\r\ncombined_data_pd \u003d combined_data.toPandas()\r\n\r\nfiltered_data \u003d combined_data_pd[(combined_data_pd[\u0027movieId\u0027] \u003e\u003d 1) \u0026 (combined_data_pd[\u0027movieId\u0027] \u003c\u003d 10)]\r\n\r\nplt.figure(figsize\u003d(10, 6))\r\nplt.bar(filtered_data[\"movieId\"], filtered_data[\"avg_rating\"])\r\nplt.xlabel(\"Movie ID\")\r\nplt.ylabel(\"Average Rating\")\r\nplt.title(\"Top 50 Movies by Average Rating (Movie IDs 1-50)\")\r\nplt.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\ntrain_data, test_data \u003d data.randomSplit([0.8, 0.2], seed\u003d123)\r\nprint(\"Number of rows in train_data:\", train_data.count())\r\nprint(\"Number of rows in test_data:\", test_data.count())\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nuser_id_input \u003d 1\r\nunrated_movies \u003d test_data.filter(test_data[\u0027userId\u0027] !\u003d user_id_input).select(\u0027movieId\u0027).distinct()\r\nunrated_movies.show(10)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.sql import Row\r\nfrom datetime import datetime\r\n\r\nratings_path \u003d \"/home/nuzul/ratings.csv\"\r\nratings_data \u003d spark.read.csv(ratings_path, header\u003dTrue, inferSchema\u003dTrue)\r\n\r\nmovie_ids_to_rate \u003d [1645, 3794, 33722, 2142, 66658, 1580, 54190, 3918, 1959]\r\nuser_id_input \u003d 1\r\nratings_from_user \u003d [4.5, 3.0, 5.0, 4.5, 3.0, 5.0, 4.5, 3.0, 5.0]\r\ncurrent_timestamp \u003d int(datetime.now().timestamp())\r\n\r\nnew_ratings \u003d []\r\nfor movie_id, rating in zip(movie_ids_to_rate, ratings_from_user):\r\n    new_ratings.append(Row(userId\u003duser_id_input, movieId\u003dmovie_id, rating\u003drating, timestamp\u003dcurrent_timestamp))\r\n\r\nnew_ratings_df \u003d spark.createDataFrame(new_ratings)\r\nnew_ratings_df.show()\r\n\r\nupdated_ratings_data \u003d ratings_data.union(new_ratings_df) \r\n\r\ntrain_data, test_data \u003d updated_ratings_data.randomSplit([0.8, 0.2], seed\u003d123)\r\nprint(\"Number of rows in train_data:\", train_data.count())\r\nprint(\"Number of rows in test_data:\", test_data.count())\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.ml.recommendation import ALS    \r\n\r\nals \u003d ALS(maxIter\u003d15, regParam\u003d0.01, userCol\u003d\"userId\", itemCol\u003d\"movieId\", ratingCol\u003d\"rating\")\r\nmodel \u003d als.fit(train_data)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\npredictions \u003d model.transform(test_data)\r\npredictions.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\npredictions_no_missing \u003d predictions.na.drop(subset\u003d[\"prediction\"])\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.ml.evaluation import RegressionEvaluator\r\n\r\nevaluator \u003d RegressionEvaluator(metricName\u003d\"rmse\", labelCol\u003d\"rating\", predictionCol\u003d\"prediction\")\r\nrmse \u003d evaluator.evaluate(predictions_no_missing)\r\nprint(f\u0027Root Mean Squared Error (RMSE): {rmse}\u0027)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nsingle_user \u003d test_data.filter(test_data[\u0027userId\u0027] \u003d\u003d 1).select([\u0027movieId\u0027, \u0027userId\u0027])\r\nsingle_user.show()\r\n\r\nreccomendations \u003d model.transform(single_user)\r\nreccomendations.orderBy(\u0027prediction\u0027, ascending\u003dFalse).show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}